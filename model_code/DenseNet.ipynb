{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"DenseNet.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"s9fvRWL4wz1P","colab_type":"code","colab":{}},"source":["from __future__ import absolute_import\n","from __future__ import division\n","from __future__ import print_function\n","\n","import tensorflow as tf"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"kcv8j6Pvo-Ri","colab_type":"code","colab":{}},"source":["class SEBlock(tf.keras.layers.Layer):\n","    def __init__(self, input_channels, r=16):\n","        super(SEBlock, self).__init__()\n","        self.pool = tf.keras.layers.GlobalAveragePooling2D()\n","        self.fc1 = tf.keras.layers.Dense(units=input_channels // r)\n","        self.fc2 = tf.keras.layers.Dense(units=input_channels)\n","\n","    def call(self, inputs, **kwargs):\n","        branch = self.pool(inputs)\n","        branch = self.fc1(branch)\n","        branch = tf.nn.relu(branch)\n","        branch = self.fc2(branch)\n","        branch = tf.nn.sigmoid(branch)\n","        branch = tf.expand_dims(input=branch, axis=1)\n","        branch = tf.expand_dims(input=branch, axis=1)\n","        output = tf.keras.layers.multiply(inputs=[inputs, branch])\n","        return output\n","\n","class ChannelAttention(tf.keras.layers.Layer):\n","    def __init__(self, in_planes, ratio=16):\n","        super(ChannelAttention, self).__init__()\n","        self.avg= tf.keras.layers.GlobalAveragePooling2D()\n","        self.max= tf.keras.layers.GlobalMaxPooling2D()\n","        self.conv1 = tf.keras.layers.Conv2D(filters = in_planes//ratio, \n","                                            kernel_size=1,\n","                                            kernel_initializer='he_normal', \n","                                            strides=1,\n","                                            padding='same')\n","        \n","        self.conv2 = tf.keras.layers.Conv2D(filters = in_planes, \n","                                            kernel_size=1,\n","                                            kernel_initializer='he_normal', \n","                                            strides=1, \n","                                            padding='same')\n","                                   \n","    def call(self, inputs):\n","        avg = self.avg(inputs)\n","        max = self.max(inputs)\n","        avg = tf.keras.layers.Reshape((1, 1, avg.shape[1]))(avg)   # shape (None, 1, 1 feature)\n","        max = tf.keras.layers.Reshape((1, 1, max.shape[1]))(max)   # shape (None, 1, 1 feature)\n","        avg_out = self.conv2(self.conv1(avg))\n","        avg_out = tf.nn.relu(avg_out)\n","        max_out = self.conv2(self.conv1(max))\n","        max_out = tf.nn.relu(max_out)\n","        out = avg_out + max_out\n","        out = tf.nn.sigmoid(out)\n","\n","        return out\n","\n","class SpatialAttention(tf.keras.layers.Layer):\n","    def __init__(self, kernel_size=7):\n","        super(SpatialAttention, self).__init__()\n","        self.conv1 = tf.keras.layers.Conv2D(filters = 1,\n","                                            kernel_size = kernel_size,\n","                                            kernel_initializer='he_normal',\n","                                            strides = 1,\n","                                            padding='same')\n","    def call(self, inputs):\n","        avg_out = tf.reduce_mean(inputs, axis=3)\n","        max_out = tf.reduce_max(inputs, axis=3)\n","        out = tf.stack([avg_out, max_out], axis=3)\n","        out = self.conv1(out)\n","        out = tf.nn.relu(out)\n","\n","        return out\n","\n","class ConvBlockAttentionModule(tf.keras.layers.Layer):\n","    def __init__(self, out_channels, ratio = 16, kernel_size = 7):\n","        super(ConvBlockAttentionModule, self).__init__()\n","        self.ca = ChannelAttention(in_planes = out_channels,\n","                                  ratio = ratio)\n","        self.sa = SpatialAttention(kernel_size= kernel_size)\n","\n","    def call(self, inputs, **kwargs):\n","      out = self.ca(inputs) * inputs\n","      out = self.sa(out) * out\n","\n","      return out\n","\n","class ConvBlock(tf.keras.Model):\n","  def __init__(self, num_filters, data_format, weight_decay=1e-4,\n","               dropout_rate=0, use_se=False, use_cbam=False):\n","    super(ConvBlock, self).__init__()\n","\n","    axis = -1 if data_format == \"channels_last\" else 1\n","    inter_filter = num_filters * 4\n","\n","    #Convolution Attention Block\n","    self.use_se = use_se\n","    self.se_block = SEBlock(input_channels=num_filters)\n","    \n","    self.use_cbam = use_cbam\n","    self.cbam_block = ConvBlockAttentionModule(out_channels=num_filters)\n","\n","    # don't forget to set use_bias=False when using batchnorm\n","    self.conv1 = tf.keras.layers.Conv2D(inter_filter,\n","                                        (1, 1),\n","                                        padding=\"same\",\n","                                        #use_bias=False,\n","                                        #data_format=data_format,\n","                                        kernel_initializer=\"he_normal\",\n","                                        #kernel_regularizer=l2(weight_decay)\n","                                        )\n","    self.dropout = tf.keras.layers.Dropout(dropout_rate)\n","\n","    self.conv2 = tf.keras.layers.Conv2D(num_filters,\n","                                        (3, 3),\n","                                        padding=\"same\",\n","                                        #use_bias=False,\n","                                        #data_format=data_format,\n","                                        kernel_initializer=\"he_normal\",\n","                                        #kernel_regularizer=l2(weight_decay)\n","                                        )\n","    self.batchnorm1 = tf.keras.layers.BatchNormalization(axis=axis)\n","    self.batchnorm2 = tf.keras.layers.BatchNormalization(axis=axis)\n","\n","\n","  def call(self, inputs, training=True):\n","    x = self.batchnorm1(inputs, training=training)\n","\n","    x = self.conv1(tf.nn.relu(x))\n","    x = self.batchnorm2(x, training=training)\n","\n","    x = self.conv2(tf.nn.relu(x))\n","    x = self.dropout(x, training=training)\n","\n","    if self.use_se == True:\n","      x = self.se_block(x)\n","    if self.use_cbam == True:\n","      x = self.cbam_block(x)\n","\n","    return x\n","\n","\n","class TransitionBlock(tf.keras.Model):\n","  def __init__(self, num_filters, data_format,\n","               weight_decay=1e-4, dropout_rate=0):\n","    super(TransitionBlock, self).__init__()\n","    axis = -1 if data_format == \"channels_last\" else 1\n","\n","    self.batchnorm = tf.keras.layers.BatchNormalization(axis=axis)\n","    self.conv = tf.keras.layers.Conv2D(num_filters,\n","                                       (1, 1),\n","                                       padding=\"same\",\n","                                       #use_bias=False,\n","                                       #data_format=data_format,\n","                                       kernel_initializer=\"he_normal\",\n","                                       #kernel_regularizer=l2(weight_decay)\n","                                       )\n","    self.avg_pool = tf.keras.layers.AveragePooling2D(data_format=data_format)\n","\n","  def call(self, inputs, training=True):\n","    x = self.batchnorm(inputs, training=training)\n","    x = self.conv(tf.nn.relu(x))\n","    x = self.avg_pool(x)\n","    return x\n","\n","\n","class DenseBlock(tf.keras.Model):\n","  def __init__(self, num_layers, growth_rate, data_format,\n","               weight_decay=1e-4, dropout_rate=0, use_se=False, use_cbam=False):\n","    super(DenseBlock, self).__init__()\n","    self.num_layers = num_layers\n","    self.axis = -1 if data_format == \"channels_last\" else 1\n","\n","    self.blocks = []\n","    for _ in range(int(self.num_layers)):\n","      self.blocks.append(ConvBlock(growth_rate,\n","                                   data_format,\n","                                   weight_decay,\n","                                   dropout_rate,\n","                                   use_se,\n","                                   use_cbam))\n","\n","  def call(self, inputs, training=True):\n","    for i in range(int(self.num_layers)):\n","      if i == 0:\n","        x = self.blocks[i](inputs, training=training)\n","      else:\n","        x = self.blocks[i](x, training=training)\n","      x = tf.concat([x, inputs], axis=self.axis)\n","\n","    return x\n","\n","\n","class DenseNet(tf.keras.Model):\n","  def __init__(self, growth_rate, output_classes, num_of_blocks=None, \n","               num_layers_in_each_block=None, data_format=\"channels_last\", \n","               compression=0.5, weight_decay=1e-4, dropout_rate=0., \n","               include_top=True, use_se=False, use_cbam=False):\n","    super(DenseNet, self).__init__() \n","    self.growth_rate = growth_rate\n","    self.num_of_blocks = num_of_blocks\n","    self.output_classes = output_classes\n","    self.num_layers_in_each_block = num_layers_in_each_block\n","    self.data_format = data_format\n","    self.compression = compression\n","    self.weight_decay = weight_decay\n","    self.dropout_rate = dropout_rate\n","    self.include_top = include_top\n","    self.use_se = use_se\n","    self.use_cbam = use_cbam\n","\n","    axis = -1 if self.data_format == \"channels_last\" else 1\n","\n","    self.num_filters = 2 * self.growth_rate\n","\n","    # first conv and pool layer\n","    self.conv1 = tf.keras.layers.Conv2D(self.num_filters,\n","                                        (7, 7),\n","                                        strides=(2, 2),\n","                                        padding=\"same\",\n","                                        #use_bias=False,\n","                                        #data_format=self.data_format,\n","                                        kernel_initializer=\"he_normal\",\n","                                        #kernel_regularizer=l2(self.weight_decay)\n","                                        )\n","    \n","    self.pool1 = tf.keras.layers.MaxPooling2D(pool_size=(3, 3),\n","                                              strides=(2, 2),\n","                                              padding=\"same\",\n","                                              data_format=self.data_format)\n","    self.batchnorm1 = tf.keras.layers.BatchNormalization(axis=axis)\n","\n","    self.batchnorm2 = tf.keras.layers.BatchNormalization(axis=axis)\n","\n","    # calculating the number of filters after each block\n","    num_filters_after_each_block = [self.num_filters]\n","    for i in range(1, self.num_of_blocks):\n","      temp_num_filters = num_filters_after_each_block[i-1] + (\n","          self.growth_rate * self.num_layers_in_each_block[i-1])\n","      # using compression to reduce the number of inputs to the\n","      # transition block\n","      temp_num_filters = int(temp_num_filters * compression)\n","      num_filters_after_each_block.append(temp_num_filters)\n","\n","    # dense block initialization\n","    self.dense_blocks = []\n","    self.transition_blocks = []\n","    for i in range(self.num_of_blocks):\n","      self.dense_blocks.append(DenseBlock(self.num_layers_in_each_block[i],\n","                                          self.growth_rate,\n","                                          self.data_format,\n","                                          self.weight_decay,\n","                                          self.dropout_rate,\n","                                          self.use_se,\n","                                          self.use_cbam))\n","      if i+1 < self.num_of_blocks:\n","        self.transition_blocks.append(\n","            TransitionBlock(num_filters_after_each_block[i+1],\n","                            self.data_format,\n","                            self.weight_decay,\n","                            self.dropout_rate))\n","\n","    # last pooling and fc layer\n","    if self.include_top:\n","      self.last_pool = tf.keras.layers.GlobalAveragePooling2D(\n","          data_format=self.data_format)\n","      self.classifier = tf.keras.layers.Dense(self.output_classes)\n","\n","  def call(self, inputs, training=None, cam=None):\n","    x = self.conv1(inputs)\n","    x = self.batchnorm1(x, training=training)\n","    x = tf.nn.relu(x)\n","    x = self.pool1(x)\n","\n","    for i in range(self.num_of_blocks - 1):\n","      x = self.dense_blocks[i](x, training=training)\n","      x = self.transition_blocks[i](x, training=training)\n","\n","    x = self.dense_blocks[self.num_of_blocks - 1](x, training=training)\n","    x = self.batchnorm2(x, training=training)\n","    x_cam = tf.nn.relu(x)\n","\n","    if self.include_top:\n","      x = self.last_pool(x_cam)\n","      x = self.classifier(x)\n","\n","    if cam == 'grad':\n","      return x_cam, x\n","\n","    return x"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"c0R0-urrttNj","colab_type":"code","colab":{}},"source":["def densenet_121(output_classes):\n","  return DenseNet(growth_rate=32, output_classes=output_classes, num_of_blocks=4, \n","num_layers_in_each_block=[6, 12, 24, 16], dropout_rate=0.3)\n","\n","def densenet_169(output_classes):\n","  return DenseNet(growth_rate=32, output_classes=output_classes, num_of_blocks=4, \n","num_layers_in_each_block=[6, 12, 32, 32], data_format=None, dropout_rate=0.4)\n","\n","def densenet_201(output_classes):\n","  return DenseNet(growth_rate=32, output_classes=output_classes, num_of_blocks=4, \n","num_layers_in_each_block=[6, 12, 48, 32], data_format=None, dropout_rate=0.5)\n","\n","def densenet_265(output_classes):\n","  return DenseNet(growth_rate=32, output_classes=output_classes, num_of_blocks=4, \n","num_layers_in_each_block=[6, 12, 64, 48], data_format=None, dropout_rate=0.5)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"yeeaVzT60YrZ","colab_type":"code","colab":{}},"source":["def se_densenet_121(output_classes):\n","  DenseNet(growth_rate=32, output_classes=output_classes, num_of_blocks=4, \n","num_layers_in_each_block=[6, 12, 24, 16], data_format=None, use_se=True)\n","\n","def se_densenet_169(output_classes):\n","  DenseNet(growth_rate=32, output_classes=output_classes, num_of_blocks=4, \n","num_layers_in_each_block=[6, 12, 32, 32], data_format=None, use_se=True)\n","\n","def se_densenet_201(output_classes):\n","  DenseNet(growth_rate=32, output_classes=output_classes, num_of_blocks=4, \n","num_layers_in_each_block=[6, 12, 48, 32], data_format=None, use_se=True)\n","\n","def se_densenet_265(output_classes):\n","  DenseNet(growth_rate=32, output_classes=output_classes, num_of_blocks=4, \n","num_layers_in_each_block=[6, 12, 64, 48], data_format=None, use_se=True)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"soLjXmS4g4XR","colab_type":"code","colab":{}},"source":["def cbam_densenet_121(output_classes):\n","  DenseNet(growth_rate=32, output_classes=output_classes, num_of_blocks=4, \n","num_layers_in_each_block=[6, 12, 24, 16], data_format=None, use_cbam=True)\n","\n","def cbam_densenet_169(output_classes):\n","  DenseNet(growth_rate=32, output_classes=output_classes, num_of_blocks=4, \n","num_layers_in_each_block=[6, 12, 32, 32], data_format=None, use_cbam=True)\n","\n","def cbam_densenet_201(output_classes):\n","  DenseNet(growth_rate=32, output_classes=output_classes, num_of_blocks=4, \n","num_layers_in_each_block=[6, 12, 48, 32], data_format=None, use_cbam=True)\n","\n","def cbam_densenet_265(output_classes):\n","  DenseNet(growth_rate=32, output_classes=output_classes, num_of_blocks=4, \n","num_layers_in_each_block=[6, 12, 64, 48], data_format=None, use_cbam=True)"],"execution_count":0,"outputs":[]}]}