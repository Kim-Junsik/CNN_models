{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"multiclass_model_tuning.ipynb","provenance":[],"collapsed_sections":[],"machine_shape":"hm","authorship_tag":"ABX9TyP40r2QqD24GLU8rgt8rhtr"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"VZyT6I_pYtsq","colab_type":"code","colab":{}},"source":["!pip install -q tensorflow-gpu==2.0.0-rc1"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"QZG27hfSY9Qr","colab_type":"code","colab":{}},"source":["!pip install tf-nightly-gpu"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Ja9D4eMpZqOK","colab_type":"code","colab":{}},"source":["!pip install import_ipynb"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"-22a0KoOZBMd","colab_type":"code","colab":{}},"source":["# mount to your google drive\n","from google.colab import drive\n","\n","drive.mount('/content/gdrive', force_remount=True)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"vXS4MYDwd_gT","colab_type":"code","colab":{}},"source":["#ipynb 모듈 불러오기 위한 설정(model import)\n","import sys\n","sys.path.append('/content/gdrive/My Drive')\n","sys.path"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"YyrPzNotZEAE","colab_type":"code","colab":{}},"source":["from __future__ import absolute_import, division, print_function, unicode_literals\n","\n","import tensorflow as tf\n","from tensorflow import keras\n","from tensorflow.keras.preprocessing import image\n","from tensorflow.keras.utils import Sequence\n","\n","from sklearn.metrics import classification_report, confusion_matrix\n","from sklearn.model_selection import train_test_split\n","from PIL import Image\n","\n","import math\n","import glob\n","import os\n","import numpy as np\n","import cv2\n","import pickle\n","\n","import matplotlib.pyplot as plt\n","import albumentations as A\n","import seaborn as sns\n","\n","import import_ipynb\n","from model_class_cp import EFFICIENTNET_B0_7, MobileNet_V1, VggNet, ResNet, DenseNet\n","\n","print(tf.__version__)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"05AcJQ3fNMqR","colab_type":"text"},"source":["## 설정(텐서, 배치, 스텝, 에폭, csv 등.)"]},{"cell_type":"code","metadata":{"id":"UbR1SJXZNNG7","colab_type":"code","colab":{}},"source":["#train, test, sample csv 읽어오기\n","train = pd.read_csv('/content/gdrive/My Drive/cvpr_data/train.csv', index_col=None)\n","test = pd.read_csv('/content/gdrive/My Drive/cvpr_data/test.csv', index_col=None)\n","sample = pd.read_csv('/content/gdrive/My Drive/cvpr_data/sample_submission.csv', index_col=None)\n","\n","#root_path 경로 설정.\n","ROOT_PATH = '/content/gdrive/My Drive/cvpr_data/images'\n","\n","# 이미지 설정(train : 1821, test : 1821)\n","IMAGE_SHAPE = [512, 768, 3]\n","\n","train_batch = 10\n","val_batch = 10\n","test_batch = 5\n","\n","train_step = 80\n","val_step = 20\n","epoch = 100\n","history = {}\n","\n","#classes\n","NUM_CLASSES = 4\n","CLASSES = ['healthy', 'multiple_diseases', 'rust', 'scab']"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"B9It1ItZzyzx","colab_type":"text"},"source":["## 모델 선택(해당 모델 번호 입력) "]},{"cell_type":"code","metadata":{"id":"ZoWrXTkKtH9-","colab_type":"code","colab":{}},"source":["def get_models(model_select):\n","  if model_select == 1:\n","    return VggNet.vgg_11(NUM_CLASSES)\n","  elif model_select == 2:\n","    return VggNet.vgg_13(NUM_CLASSES)\n","  elif model_select == 3:\n","    return VggNet.vgg_16(NUM_CLASSES)\n","  elif model_select == 4:\n","    return VggNet.vgg_19(NUM_CLASSES)\n","  elif model_select == 5:\n","    return VggNet.se_vgg_16(NUM_CLASSES)\n","  elif model_select == 6:\n","    return VggNet.se_vgg_19(NUM_CLASSES)\n","  elif model_select == 7:\n","    return ResNet.resnet_18(NUM_CLASSES)\n","  elif model_select == 8:\n","    return ResNet.resnet_34(NUM_CLASSES)\n","  elif model_select == 9:\n","    return ResNet.resnet_50(NUM_CLASSES)\n","  elif model_select == 10:\n","    return ResNet.resnet_101(NUM_CLASSES)\n","  elif model_select == 11:\n","    return ResNet.resnet_152(NUM_CLASSES)\n","  elif model_select == 12:\n","    return ResNet.se_resnet_50(NUM_CLASSES)\n","  elif model_select == 13:\n","    return ResNet.se_resnet_101(NUM_CLASSES)\n","  elif model_select == 14:\n","    return ResNet.se_resnet_152(NUM_CLASSES)\n","  elif model_select == 15:\n","    return DenseNet.DenseNet_121(NUM_CLASSES)\n","  elif model_select == 16:\n","    return DenseNet.DenseNet_169(NUM_CLASSES)\n","  elif model_select == 17:\n","    return DenseNet.DenseNet_201(NUM_CLASSES)\n","  elif model_select == 18:\n","    return DenseNet.DenseNet_265(NUM_CLASSES)\n","  elif model_select == 19:\n","    return DenseNet.se_DenseNet_121(NUM_CLASSES)\n","  elif model_select == 20:\n","    return DenseNet.se_DenseNet_169(NUM_CLASSES)\n","  elif model_select == 21:\n","    return DenseNet.se_DenseNet_201(NUM_CLASSES)\n","  elif model_select == 22:\n","    return DenseNet.se_DenseNet_265(NUM_CLASSES)\n","  elif model_select == 23:\n","    return EFFICIENTNET_B0_7.efficient_net_b0(NUM_CLASSES)\n","  elif model_select == 24:\n","    return EFFICIENTNET_B0_7.efficient_net_b1(NUM_CLASSES)\n","  elif model_select == 25:\n","    return EFFICIENTNET_B0_7.efficient_net_b2(NUM_CLASSES)\n","  elif model_select == 26:\n","    return EFFICIENTNET_B0_7.efficient_net_b3(NUM_CLASSES)\n","  elif model_select == 27:\n","    return EFFICIENTNET_B0_7.efficient_net_b4(NUM_CLASSES)\n","  elif model_select == 28:\n","    return EFFICIENTNET_B0_7.efficient_net_b5(NUM_CLASSES)\n","  elif model_select == 29:\n","    return EFFICIENTNET_B0_7.efficient_net_b6(NUM_CLASSES)\n","  elif model_select == 30:\n","    return EFFICIENTNET_B0_7.efficient_net_b7(NUM_CLASSES)\n","  elif model_select == 31:\n","    return MobileNet_V1.MobileNet_V1(NUM_CLASSES)\n","  elif model_select == 32:\n","    return MobileNet_V1.se_MobileNet_V1(NUM_CLASSES)\n","  else:\n","    raise ValueError(\"The model_index does not exist.\")"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"TfFpW_dMwhdQ","colab_type":"code","outputId":"d3c12ffb-28bd-4a5c-8c0d-076997241a98","executionInfo":{"status":"ok","timestamp":1585222576098,"user_tz":-540,"elapsed":2306,"user":{"displayName":"정규준","photoUrl":"","userId":"06773425004265186105"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["model_select = int(input('Please enter the model index you want to use.'))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Please enter the model index you want to use.3\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"zxsIeKjRvLwe","colab_type":"code","colab":{}},"source":["get_model = get_models(model_select)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"KXmRQFSINZUe","colab_type":"text"},"source":["## Dataset Directory & Data loader Implementation  "]},{"cell_type":"code","metadata":{"id":"LeqSK-VNqKXW","colab_type":"code","colab":{}},"source":["#albumentations Module 설정.\n","transforms_train = A.Compose([\n","    A.Resize(height=IMAGE_SHAPE[0], width=IMAGE_SHAPE[1], p=1.0),\n","    A.RandomCrop(height=IMAGE_SHAPE[0]-2, width=IMAGE_SHAPE[1]-2, p=1.0),\n","    A.Resize(height=IMAGE_SHAPE[0], width=IMAGE_SHAPE[1], p=1.0),\n","    A.Flip(),\n","    A.ShiftScaleRotate(rotate_limit=40.0, p=0.8),\n","    A.HorizontalFlip(p=0.5)\n","])\n","\n","transforms_valid = A.Compose([\n","    A.Resize(height=IMAGE_SHAPE[0], width=IMAGE_SHAPE[1], p=1.0)\n","])"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"WyH6x_e5qODP","colab_type":"code","colab":{}},"source":["images = []\n","test_images = []\n","\n","image_ids = list(train.image_id)\n","test_ids = list(test.image_id)\n","\n","for image_id in image_ids:\n","  images.append(os.path.join(ROOT_PATH,image_id) + '.jpg')\n","for test_id in test_ids:\n","  test_images.append(os.path.join(ROOT_PATH, test_id) + '.jpg')\n","\n","labels = list(zip(train.healthy, train.multiple_diseases, train.rust, train.scab))\n","test_labels = []\n","\n","train_images, val_images, train_labels, val_labels = train_test_split(images, labels, test_size = 0.2, random_state=2020, stratify=labels)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ghTCr8ZhqTEG","colab_type":"code","colab":{}},"source":["print(len(train_images), len(val_images), len(train_labels), len(val_images))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"fkL1NVuqqVbb","colab_type":"code","colab":{}},"source":["train_images[0], val_images[0]"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"FBthXKgQqZqw","colab_type":"code","colab":{}},"source":["img = plt.imread(train_images[0])\n","print(img.shape)\n","plt.imshow(img)\n","\n","img = plt.imread(val_images[0])\n","print(img.shape)\n","plt.imshow(img)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"65s61O-xvm7t","colab_type":"code","colab":{}},"source":["# 데이터 확인 함수\n","def check_images(labels, classes):\n","  healthy = labels.healthy\n","  multiple_diseases = labels.multiple_diseases\n","  rust =labels.rust\n","  scab= labels.scab\n","\n","  num_healthy = [len([i for i in healthy if i == 1]), len([i for i in healthy if i == 0])]\n","  num_multiple = [len([i for i in multiple_diseases if i == 1]), len([i for i in multiple_diseases if i == 0])]\n","  num_rust = [len([i for i in rust if i == 1]), len([i for i in rust if i == 0])]\n","  num_scab = [len([i for i in scab if i == 1]), len([i for i in scab if i == 0])]\n","\n","  pos = [num_healthy[0], num_multiple[0], num_rust[0], num_scab[0]]\n","  nev = [num_healthy[1], num_multiple[1], num_rust[1], num_scab[1]]\n","\n","  plt.rcParams[\"font.size\"] = 12\n","\n","  plt.figure(figsize=(12,8))\n","\n","  x = np.arange(len(classes))\n","  p1 = plt.bar(x-0.15, pos, width=0.3, color='#FF0000', label = 1, alpha=0.5)\n","  plt.xticks(x, classes)\n","  p2 = plt.bar(x+0.15, nev, width=0.3, color='#0000FF', label = 0, alpha = 0.5)\n","  plt.xticks(x, classes)\n","\n","  for i, rect in enumerate(p1):\n","      plt.text(rect.get_x() + rect.get_width() / 2.0, 0.95 * rect.get_height(), str(pos[i]), ha='center')\n","  for i, rect in enumerate(p2):\n","      plt.text(rect.get_x() + rect.get_width() / 2.0, 0.95 * rect.get_height(), str(nev[i]), ha='center')\n","  plt.legend((p1[0], p2[0]), ('1','0'), fontsize = 15)\n","\n","  plt.show()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"028HIRoLpX3P","colab_type":"code","colab":{}},"source":["check_images(train, CLASSES)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"rx4Ni2_emZz5","colab_type":"code","colab":{}},"source":["class Classification_Dataset:\n","    # class_list -> image_labels 변경\n","    def __init__(self, image_paths = \"\", image_labels = \"\", target_size = None, augment = None):\n","        \n","        self.image_list = image_paths\n","        self.image_labels = image_labels\n","        self.target_size = target_size\n","        self.augment = augment\n","        #self.num_of_class = len(self.class_list)\n","    \n","    \n","    def __len__(self):\n","        return len(self.image_list)\n","\n","    \n","    def __getitem__(self, idx):\n","        image = cv2.imread(self.image_list[idx])\n","        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n","        image = image/255.0\n","        label = self.image_labels[idx]\n","        \n","        #for i in range(self.num_of_class):\n","        #    if(self.class_list[i] in self.image_list[idx]):\n","        #        label = i\n","        #        break             \n","\n","        \n","        if self.augment:\n","            transformed = self.augment(image=image)\n","            image = transformed['image']\n","\n","        #image = image.resize(self.target_size)\n","        return image, label"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"GB6MMS8JmcRE","colab_type":"code","colab":{}},"source":["class Classification_Data_Loader(Sequence):\n","\n","    def __init__(self, dataset, batch_size=None, shuffle=False):\n","        \n","        self.dataset = dataset\n","        self.batch_size = batch_size\n","        self.shuffle=shuffle\n","        self.index_list = [idx for idx in range(len(self.dataset))]\n","        self.idx=0\n","        \n","    def __getitem__(self, idx):\n","      \n","        start = idx * self.batch_size\n","        end = (idx+1) * self.batch_size\n","        data = []\n","        label = []\n","        \n","        if self.shuffle:\n","            np.random.shuffle(self.index_list)\n","            \n","        for j in range(start,end):\n","            if j >= len(self.index_list):\n","                j%=len(self.dataset)\n","            data.append(self.dataset[self.index_list[j]])\n","      \n","        batch = tuple(tf.stack(sample, axis=0) for sample in zip(*data))\n","\n","        if self.idx >= (len(self.dataset)//self.batch_size):\n","            self.idx=0\n","        self.idx +=1\n","        return batch\n","\n","    def __call__(self):\n","        batch = self.__getitem__(self.idx)\n","        return batch\n","\n","    def __len__(self):\n","        return (len(self.dataset) // self.batch_size)\n","\n","    def get_batch(self):\n","        return self.batch_size"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"MJU6FaLgwIl_","colab_type":"code","colab":{}},"source":["# 데이터 로더 동작 확인\n","def data_info_print(data_gen, idx):\n","  batch = data_gen.__getitem__(idx)\n","  print(type(batch), type(batch[0]), type(batch[1]))\n","  # 변경\n","  print(len(batch[0]), len(batch[1]))\n","  plt.figure(figsize=(20,8))\n","  for i in range(len(batch[0])):\n","    plt.subplot(2,5,i+1)\n","    # 변경 \n","    plt.imshow(batch[0][i])\n","    \n","  print('label :', batch[1])\n","  print(type(batch[0][0]), type(batch[1][0]))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"iFQghyCd0YyE","colab_type":"text"},"source":["# 데이터 확인 & 데이터 로더 생성\n","\n"]},{"cell_type":"code","metadata":{"id":"48vc2yknmU6k","colab_type":"code","colab":{}},"source":["train_dataset = Classification_Dataset(image_paths=train_images, image_labels = train_labels, augment = transforms_train)\n","valid_dataset = Classification_Dataset(image_paths=val_images, image_labels = val_labels, augment = transforms_valid)\n","\n","train_data_gen = Classification_Data_Loader(dataset=train_dataset, batch_size=train_batch, shuffle=True)\n","valid_data_gen = Classification_Data_Loader(dataset=valid_dataset, batch_size=val_batch, shuffle=True)\n","\n","images, labels = train_data_gen.__getitem__(0)\n","print(labels)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"vlXjaipdwy1W","colab_type":"code","colab":{}},"source":["data_info_print(train_data_gen,0)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"F_HSzCeK0g9p","colab_type":"text"},"source":["# 모델 훈련  \n","### Using GradientTape\n"]},{"cell_type":"code","metadata":{"id":"8CmrireBw5q1","colab_type":"code","colab":{}},"source":["def fit_test(model, train_gen, train_steps, epochs, val_gen, val_steps):\n","    BATCH_SIZE = train_gen.get_batch()\n","  # define loss and optimizer\n","    optimizer = keras.optimizers.Adam(learning_rate=0.0000779)\n","    loss_func = keras.losses.CategoricalCrossentropy()\n","\n","    train_loss = tf.keras.metrics.Mean(name='train_loss')\n","    train_accuracy = tf.keras.metrics.CategoricalAccuracy(name='train_accuracy')\n","\n","    valid_loss = tf.keras.metrics.Mean(name='valid_loss')\n","    valid_accuracy = tf.keras.metrics.CategoricalAccuracy(name='valid_accuracy')\n","\n","\n","    train_losses = []\n","    train_acces = []\n","    val_losses = []\n","    val_acces = []\n","\n","\n","    @tf.function\n","    def train_step(images, labels):\n","        with tf.GradientTape() as tape:\n","            predictions = model(images, training=True)\n","            loss = loss_func(y_true=labels, y_pred=predictions)\n","        gradients = tape.gradient(loss, model.trainable_variables)\n","        optimizer.apply_gradients(grads_and_vars=zip(gradients, model.trainable_variables))\n","\n","        train_loss(loss)\n","        train_accuracy(labels, predictions)\n","\n","    @tf.function\n","    def valid_step(images, labels):\n","\n","        predictions = model(images, training=False)\n","        v_loss = loss_func(labels, predictions)\n","\n","        valid_loss(v_loss)\n","        valid_accuracy(labels, predictions)\n","\n","    def step_decay(epoch, k):\n","        if epoch < k :\n","          return True\n","        #연속으로 k-1번 떨어지면 learning rate 조정.\n","        F_len = 0\n","\n","        for step_k in range(k, 1, -1):\n","          step_k = -step_k\n","          if val_losses[step_k + 1] > val_losses[step_k]:\n","            #print('k+1 : ', val_losses[step_k + 1], ' > ', 'k : ', val_losses[step_k])\n","            F_len = F_len + 1\n","\n","        if F_len == k-1:\n","          if 1e-06 >= optimizer.learning_rate:\n","            optimizer.learning_rate = 1e-06\n","          else:\n","            optimizer.learning_rate = optimizer.learning_rate - (optimizer.learning_rate / 5)\n","          \n","          print('step_decay is reduce %6.7f.' % (optimizer.learning_rate))\n","\n","  # start training\n","    for epoch in range(epochs):\n","        step_decay(epoch, 4)\n","        \n","        train_loss.reset_states()\n","        train_accuracy.reset_states()\n","        valid_loss.reset_states()\n","        valid_accuracy.reset_states()\n","        #step = 0\n","        for step in range(train_steps):\n","            print('.',end='')\n","            if step % 100 == 0:\n","                print()\n","            images, labels = train_gen()\n","            train_step(images, labels)\n","            \n","        print('x')\n","        \n","        for val_step in range(val_steps):\n","            print('.', end='')\n","            if val_step % 100 == 0:\n","                print()\n","            valid_images, valid_labels = val_gen()\n","            valid_step(valid_images, valid_labels)\n","        print('x')\n","        print(\"Epoch: {}/{}, train loss: {:.5f}, train accuracy: {:.5f}, \"\n","              \"valid loss: {:.5f}, valid accuracy: {:.5f}\".format(epoch + 1,\n","                                                                epochs,\n","                                                                train_loss.result(),\n","                                                                train_accuracy.result(),\n","                                                                valid_loss.result(),\n","                                                                valid_accuracy.result()))\n","        train_losses.append(train_loss.result())\n","        train_acces.append(train_accuracy.result())\n","        val_losses.append(valid_loss.result())\n","        val_acces.append(valid_accuracy.result())\n","\n","    history = {'train_losses': train_losses, 'train_acces': train_acces, 'val_losses': val_losses, 'val_acces': val_acces}\n","    return history"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"eAnBLmbkw6Pk","colab_type":"code","colab":{}},"source":[" def test_eval(model, test_data_gen, test_steps):\n","  loss_func = keras.losses.BinaryCrossentropy()\n","  test_loss = tf.keras.metrics.Mean()\n","  test_accuracy = tf.keras.metrics.BinaryAccuracy()\n","\n","  @tf.function\n","  def test_step(images, labels):\n","      predictions = model(images, training=False)\n","      t_loss = loss_func(labels, predictions)  \n","\n","      test_loss(t_loss)\n","      test_accuracy(labels, predictions)\n","\n","  for step in range(test_steps):\n","    print('.',end='')\n","    if step % 100 == 0:\n","      print()\n","    test_images, test_labels = test_data_gen()\n","    test_step(test_images, test_labels)\n","  print('x')\n","  print(\"loss: {:.5f}, test accuracy: {:.5f}\".format(test_loss.result(),\n","                                                      test_accuracy.result()))\n","\n","  #print(\"The accuracy on test set is: {:.3f}%\".format(test_accuracy.result()*100))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"j23bzUsTw75C","colab_type":"code","colab":{}},"source":["get_model.build(input_shape=(None, IMAGE_SHAPE[0], IMAGE_SHAPE[1], IMAGE_SHAPE[2]))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"DHBrGQMtxaPP","colab_type":"code","colab":{}},"source":["get_model.summary()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ToyY5HaSxcxJ","colab_type":"code","colab":{}},"source":["history = fit_test(model=get_model, \n","                   train_gen=train_data_gen, \n","                   train_steps=train_step, \n","                   epochs=epoch, \n","                   val_gen=valid_data_gen, \n","                   val_steps=val_step)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"jzRy9pe10oqt","colab_type":"text"},"source":["# 저장"]},{"cell_type":"code","metadata":{"id":"i2A_deCzxosH","colab_type":"code","colab":{}},"source":["get_model.save_weights('/content/gdrive/My Drive/get_model', save_format='tf')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"dI3agLw4q-ho","colab_type":"code","colab":{}},"source":["with open('/content/gdrive/My Drive/junsick_coding/model_history.bin', 'wb') as hi:\n","  pickle.dump(history, hi)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"wBgE6YcAAHn_","colab_type":"text"},"source":["## 로드"]},{"cell_type":"code","metadata":{"id":"qhOaBwIRAJMG","colab_type":"code","colab":{}},"source":["get_model.load_weights('/content/gdrive/My Drive/get_model')   "],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"_zu4a6udrQrG","colab_type":"code","colab":{}},"source":["with open('/content/gdrive/My Drive/junsick_coding/model_history.bin', 'rb') as f:\n","  re_history = pickle.load(f)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ioXEZwcf0q7Z","colab_type":"text"},"source":["# 모델 학습 가시화 그래프\n"]},{"cell_type":"code","metadata":{"id":"km0nmVryxvq7","colab_type":"code","colab":{}},"source":["def his_graph(history, epoch):\n","  acc = history['train_acces']\n","  val_acc = history['val_acces']\n","\n","  loss = history['train_losses']\n","  val_loss = history['val_losses']\n","\n","  epochs_range = range(epoch)\n","\n","  plt.figure(figsize=(8, 8))\n","  plt.subplot(2, 1, 1)\n","  plt.plot(epochs_range, acc, label='Training Accuracy')\n","  plt.plot(epochs_range, val_acc, label='Validation Accuracy')\n","  plt.legend(loc='lower right')\n","  plt.title('Training and Validation Accuracy')\n","\n","  plt.subplot(2, 1, 2)\n","  plt.plot(epochs_range, loss, label='Training Loss')\n","  plt.plot(epochs_range, val_loss, label='Validation Loss')\n","  plt.legend(loc='upper right')\n","  plt.title('Training and Validation Loss')\n","\n","  plt.show()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"U_NAHJelxx1g","colab_type":"code","colab":{}},"source":["his_graph(history, epoch)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"FeuQ9fP8ckyo","colab_type":"text"},"source":["#모델 예측 및 가시화(confusion matrix)"]},{"cell_type":"code","metadata":{"id":"ASJEoHFJxz5e","colab_type":"code","colab":{}},"source":["def model_predict(model, test_gen, steps):\n","  predics=[]\n","  labels = []\n","  for step in range(steps):\n","    image,label = test_gen()\n","    pred = model(image)\n","    labels.append(label)\n","    predics.append(pred)\n","  re_dic = dict({'pred':predics, 'labels':labels})\n","  return re_dic"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"cscbzdLmx3HI","colab_type":"code","colab":{}},"source":["pred_dataset = Dataset_dir_ver(path=test_PATH)\n","pred_data_gen = Data_Loader(dataset=test_dataset, batch_size=1)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"UZ02piGnx3r3","colab_type":"code","colab":{}},"source":["def pred_confusion_matrix(pred, labels):\n","  pred_int = []\n","  for i in range(len(pred)):\n","    if pred[i] > 0.5:\n","      pred_int.append(1)\n","    elif pred[i] <= 0.5:\n","      pred_int.append(0)\n","\n","  print(len(pred))\n","  print(len(pred_int))\n","  print('Confusion Matrix')\n","  conf_matrix = confusion_matrix(labels, pred_int)\n","  print(conf_matrix)\n","  sns.heatmap(conf_matrix,cmap=\"Blues\",annot=True,fmt='g');\n","  plt.xlabel('predicted value')\n","  plt.ylabel('true value');"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"A2o2Hj5jx72a","colab_type":"code","colab":{}},"source":["re_dic = model_predict(get_model, pred_data_gen, 4000)\n","pred = re_dic['pred']\n","labels = re_dic['labels']\n","pred = list(map(float,pred))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"TB7AY_CCx8xA","colab_type":"code","colab":{}},"source":["pred_confusion_matrix(pred, labels)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"kKoUWvCYcv-N","colab_type":"text"},"source":["#Grad CAM\n","######알맞게 모델 조정 필요(return output, mask)"]},{"cell_type":"code","metadata":{"id":"xZLLcjobbOmN","colab_type":"code","colab":{}},"source":["def grad_cam(model, idx, class_name):\n","  plt.figure(figsize=(18, 15))\n","  for i in range(40,50):\n","    path = os.path.join('/content/gdrive/My Drive/cvpr_data/images/Train_%d.jpg'%i)\n","    image = cv2.imread(path)\n","    img = cv2.resize(image, (IMAGE_SHAPE[0], IMAGE_SHAPE[1]))\n","    x = img.copy()\n","    x.astype(np.float32)\n","    x = np.expand_dims(x, axis=0)\n","    x = x / 255.0\n","    \n","    #grad_model = tf.keras.models.Model(\n","    #      [resnet101.layers[0]], [resnet101.layers[-2],resnet101.layers[-1]]\n","    #  )\n","      \n","    with tf.GradientTape() as tape:\n","        inputs = tf.cast(x, tf.float32)\n","        model_outputs, predictions = model(inputs, cam = 'grad')\n","        loss = predictions[:,0]\n","\n","    grads = tape.gradient(loss, model_outputs)\n","\n","    guided_grads = (\n","          tf.cast(model_outputs > 0, \"float32\") * tf.cast(grads > 0, \"float32\") * grads\n","    )\n","\n","    prediction = predictions[0][idx]\n","    model_outputs = model_outputs[0]\n","    plt.subplot(4, 5, i-40+1)\n","    weights = np.mean(grads, axis=(1, 2))\n","    weights = weights.reshape(2048, 1)\n","\n","    cam = (prediction -0.5) * np.matmul(model_outputs, weights)\n","    cam -= np.min(cam)\n","    cam /= np.max(cam)\n","    cam -= 0.2\n","    cam /= 0.8\n","\n","    try:\n","      cam = cv2.resize(np.float32(cam), (IMAGE_SHAPE[0], IMAGE_SHAPE[1]))\n","    except Exception as e:\n","      #print(cam.shape)\n","      print(str(e))\n","\n","    heatmap = cv2.applyColorMap(np.uint8(255*cam), cv2.COLORMAP_JET)\n","    heatmap[np.where(cam <= 0.2)] = 0\n","    grad_cam = cv2.addWeighted(img, 0.8, heatmap, 0.4, 0)\n","    plt.axis('off')\n","    plt.title('%s_class_grad_cam'%class_name)\n","    plt.imshow(grad_cam[:, :, ::-1])"],"execution_count":0,"outputs":[]}]}