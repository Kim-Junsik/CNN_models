## 컨볼루션(Convolution)
* 배열 X와 W가 있을때 W를 뒤집은 W<sup>r</sup>배열에서, X와 W의 각 원소를 곱하고 더하는 연산을 진행한다.
* W<sup>r</sup>은 X를 전체 순회하며 위와 같은 연산을 진행한다.
* 딥러닝 패키지들은 합성곱을 구현할때 합성곱이 아닌 교차 상관(cross-correlation)을 사용한다.
* 교차 상관은 계산법은 동일하지만 W<sup>r</sup>이 아닌 원래 W를 사용한다.
  * 합성곱 신경망의 가중치 배열은 무작위로 초기화 되어 있다. 따라서 가중치를 뒤집어서 합성곱을 적용하던지 뒤집지 않고 교차 상관을 적용하던지 상관이 없다.

## 패딩(Padding)과 스트라이드(stride)
* 패딩(padding)은 원본 배열의 양 끝에 빈 원소를 추가하는 것을 말한다.
* 스트라이드(stride)는 미끄러지는 배열의 간격을 조절하는 것을 말한다.
* 밸리드 패딩(valid padding)
  * 원본 배열에 패딩을 추가 하지 않고 미끄러지는 배열이 원본 배열의 끝으로 갈 때까지 교차 상관을 수행한다.
  * 밸리드 패딩의 결과로 얻은 배열의 크기는 원본 배열보다 항상 작다.
  * 벨리드 패딩의 특징은 원본 배열의 각 원소가 연산에 참여하는 정도가 다르다.
  * 밸리드 패딩은 원본 배열의 양 끝의 원소는 연산의 참여 횟수가 다르다.
* 제로 패딩(zero padding)
  * 원본 배열의 모든 원소의 연산 참여 횟수가 같게 하려면 배열 양끝에 가상의 값을 추가해야 한다.
  * 이때 가상의 원소를 0으로 할 경우 이를 제로 패딩이라고 한다.
  * 적절한 개수의 제로 패딩을 추가하면 원본 배열의 모든 원소가 연산에 동일하게 참여하게 만들 수 있다.
  * 원본 배열의 모든 요소가 동일하게 연산에 참여하는 패딩 방식을 풀패딩(full padding)이라고 한다.
* 세임 패딩(same padding)
  * 출력 배열의 길이가 원본 배열의 길이와 같아지도록 원본 배열에 제로 패딩을 추가한다.

## 2차원 배열에서의 합성곱 연산
* 2차원 배열에서의 합성곱 연산은 1차원 배열과 다른 점은 수행 방행이 왼쪽에서 오른쪽으로, 위에서 아래로 이동하며 배열 원소끼리 곱하고 더하는 것이다.

## 텐서플로의 합성곱 연산
* 텐서플로에서의 합성곱은 입력값이 4차원 텐서를 기대한다
* 텐서의 각 부분은 <b>(배치, 샘플의 높이, 샘플의 너비, 채널)</b>이다.
* 가중치도 4개의 차원으로 구성되며 <b>(가중치의 높이, 가중치의 너비, 채널, 가중치의 개수)</b>이다.
* 일반적으로 합성곱의 입력과 가중치의 채널 수는 동일하다. 즉 채널 방향으로는 가중치가 이동하지 않는다.
* 입력과 가중치에 세임 패딩을 적용하여 합성곱 연산을 수행하면 출력은 <b>(입력의 배치, 입력의 높이, 입력의 너비, 가중치의 개수)</b>가 된다.
* 합성곱 신경망을 사용하면 가중치 배열의 크기는 훨씬 작이지고 입력의 특징을 더 잘 찾기 때문에 합성곱 신경망이 이미지 분류에서 뛰어난 성능을 발휘한다.
* 합성곱의 가중치는 필터(filter), 커널(kernel)이라고도 부른다.

## 풀링
* 합성곱이 일어나는 층을 합성곱층, 풀링이 일어나는 층을 풀링층이라고 한다.
* 합성곱층과 풀링층에서 만들어진 결과를 특성맵(feature map)이라고 한다.
* 입력이 합성곱층을 통과할 때 합성곱과 활성화 함수가 적용되어 특성 맵이 만들어지며, 그런 다음 특성 맵이 풀링층을 통과하여 또 다른 특성 맵이 만들어 진다.
* 풀링이란 특성 맵을 스캔하며 최댓값을 고르거나 평균값을 계산하는 것을 말한다.
* 최대 풀링(max pooling)
  * 풀링 영역에서 최댓값을 선택하는 풀링 방식이다.
  * 일반적으로 풀링의 스트라이드는 풀링의 한 모서리의 크기로 한다.
  * 즉 2*2영역의 풀링은 스트라이드가 2로 겹치지 않게 스캔한다.
  * 2*2풀링은 특성맵의 크기를 1/2로 줄인다(면적은 1/4로 줄어든다).
  * 특성맵의 크기를 절반으로 줄이면 특성 맵의 한 요소가 입력의 더 넓은 영역을 바라볼 수 있는 효과를 얻을 수 있다.
  * 합성곱 신경망의 전형적인 형태는 합성곱층 뒤에 풀링층이 뒤따르는 것이다.
* 평균 풀링(average pooling)
  * 풀링 영역의 평균값을 계산한다.
  * 연구자들은 평균 풀링보다 최대 풀링을 선호하는데 그 이유는 평균 풀링은 합성곱층을 통과하는 특징들을 희석시킬 가능성이 높기 때문이다.
  * 즉, 입력에서 합성곱 필터가 찾고자 하는 부분은 특성 맵의 가장 큰 값으로 활성화 되는데 평균 풀링은 가장 큰 특성의 값을 상쇄시키기 때문이다.
  * 풀링층에는 학습되는 가중치가 없다. 또한 풀링은 배치 차원이나 채널 차원으로 적용되지 않는다. 즉, 풀링층을 통과하기 전후로 배치 크기와 채널 크기는 동일하다.(풀링은 각 샘플마다 또 각 채널마다 독립적으로 수행된다.)

## 합성곱 신경망의 구조
* 렐루 함수(ReLU)
  * 주로 합성곱층에 적용되는 활성화 함수로, 합성곱 신경망의 성능을 더 높여준다. 렐루 함수는 0보다 큰 값은 그대로 통과시켜주며 0보다 작은 값은 0으로 만든다.
  * 렐루함수의 도함수는 0보다 크면 1이고 0보다 작으면 0이다. 렐루 함수는 0에서는 미분이 불가능하지만 딥러닝 패키지는 0에서의 도함수는 0으로 가정한다.

* 합성곱 신경망은 이미지의 2차원 형태를 입력으로 그대로 사용하므로 이미지를 한 줄로 펼칠 필요가 없다.
* 이런 특성 때문에 이미지 정보가 손상되지 않는다는 장점이 있다.
* 채널이란 이미지의 픽셀이 가진 색상을 표현하기 위해 필요한 정보를 말한다.
* 흑백 이미지의 경우 흑백의 강도만 표현하면 되므로 하나의 채널만 가진다.
* 이미지의 모든 채널에 합성곱이 한 번에 적용되어야 하므로 커널의 마지막 차원은 입력 채널의 개수와 동일해야 한다.<br/>(4 * 4 * **10**이미지 --> 3 * 3 * **10** 커널)
* 입력 채널은 커널의 채널과 각각 합성곱 연산을 수행한다. 그런 다음 함성곱의 전체 결과를 더하여 특성맵 1 조각을 만들어 낸다.
* 이미지의 여러 특성을 감지하기 위해서는 여러 개의 커널을 사용해야 한다.
* 합성곱층을 통해 특성 맵이 만들어지면 이 특성 맵에 활성화 함수로 렐루 함수를 적용하고 풀링이 적용된다.
* 풀링은 특성맵의 크기를 줄여준다. 즉, 특성 맵의 크기가 절반으로 줄어든다.(채널의 크기는 줄어들지 않는다.)
* 일반적으로 합성곱층과 풀링층을 통과한 특성 맵을 일렬로 펼쳐서 완전연결층에 입력으로 주입한다.
* 완전연결층은 한 신경망에 여러개가 들어 있을 수도 있다.
* 완전연결층은 합성곱층에서 찾은 특성을 사용하여 최종 분류단계를 수행하는 과정으로 볼 수 있다.

## 합성곱 신경망 구현
* GradientTape (Tensorflo)
  * 사용자가 작성한 연산을 계산 그래프(computation graph)로 만들어 자동 미분 기능을 구현한다.
  * 자동 미분 기능을 사용하면 임의의 파이썬 코드나 함수에 대한 미분값을 계산할 수 있다.
  * 텐서플로에서 사용하려면 with블럭으로 tf.GradientTape()객체가 감시할 코드를 감싸야한다.
  * tape객체는 with블럭 안에서 일어나는 모든 연산을 기록하고 텐서플로 변수인 tf.Variable객체를 자동으로 추적한다.
  * 그레디언트를 계산하려면 미분 대상 객체와 변수를 tape객체의 gradient()메서드에 전달해야 한다.

* 경사 하강법은 출발점에서 기울기가 0인 최저점을 찾아간다.
* 출발점이 적절하지 않은 곳에 설정되면 전역 최저점(global minimum)을 찾는 것이 아닌 지역 최소점(local minimum)을 찾을 수도 있다.
* 글로럿 초기화 방식
  * 글로럿 초기화 방식은 난수를 생성하여 가중치를 초기화 할때 다음과 같은 범위에서 균등하게 난수를 발생하여 가중치의 초기화를 진행한다

<p align="center"><img src="https://user-images.githubusercontent.com/46274774/85224658-cbc96b80-b406-11ea-9047-1d2283a987f9.png" width="35%"></p>

## 합성곱 신경망 모델(Keras)
* 배치 차원이 None인 이유는 배치 입력의 개수는 훈련할 때 전달되는 샘플 개수에 따라 달라지기 때문이다.
* 모델의 파라미터 개수는 전체 가중치의 크기와 커널마다 하나씩 절편을 추가하여 계산된다.

* 드롭아웃
  * 신경망의 과대 적합을 줄이는 방법중 하나이다.
  * 드롭아웃은 무작위로 신경망에서 하나의 뉴런을 비활성화시킨다.
  * 무작위로 일부 뉴런을 비활성화 시키면 특정 뉴런에 과도하게 의존하여 훈련하는 것을 막아준다.
  * 일부 뉴런이 비 활성화 되었을때도 예측을 잘 하려면 특정 뉴런에 과도하게 의존하지 않고 모든 뉴런이 의미있는 패턴을 학습해야 한다.
  드롭아웃은 모델을 훈련시킬 때만 적용하는 기법으로 테스트나 실전에는 사용하지 않는다.
  * 이로 인해 상대적으로 테스트와 실전의 출력값이 훈련할 때의 출력값보다 높아지므로 테스트나 실전에서는 출력값을 드롭아웃 비율만큼 낮춰야 한다.
 * 하지만 텐서플로를 비롯한 대부분의 딥러닝 프레임워크들은 이 문제를 반대로 해결한다. 즉, 훈련할 때 드롭아웃 비율만큼 뉴런의 출력을 높여 훈련 시킨다.

* 분류 문제에서 정확도를 직접 최적화할 수는 없다. 대신 크로스 엔트로피 손실 함수를 대신 최적화 한다.
* 손실 함수를 최소화하면 정확도가 높아질 것으로 기대할 수 있지만 반그시 그렇지는 않다.

