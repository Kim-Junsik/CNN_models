* 로지스틱 회귀에 대해 알기위해 우선 퍼셉트론에대해 공부한다.
* 퍼셉트론(perceptron)알고리즘은 이진 분류 문제에서 최적의 가중치를 학습하는 알고리즘이다.
* 이진 분류 문제(binary classification)란 임의의 샘플 데이터가 있을때 이를 True또는 False로 구분하는 문제를 말한다.

# 퍼셉트론
1.   퍼셉트론은 직선의 방정식을 사용해서 선형회기와 유사한 구조를 한다.
2.   퍼셉트론은 마지막 단계에서 샘플을 이진 분류하기 위해 step function을 사용하며 여기서 나온 값을 가중치와 절편의 업데이트에 사용한다.
3.   선형함수 : w<sub>1</sub>x<sub>1</sub> + w<sub>2</sub>x<sub>2</sub> + b = z    -->  사용
4.   계단함수는 z가 0보다 크거나 같으면 1, 0보다 작으면 -1로 분류한다. ==> 1은 양성 클래스(positive class) -1은 음성 클래스(negative class)
5. 여기서 w<sub>i</sub>와 x<sub>i</sub>는 각각 i번째 가중치와 특성 값으로 n개의 특성이 있다면 다음과 같이 표현할 수 있다.

    z = w<sub>1</sub>x<sub>1</sub> + w<sub>2</sub>x<sub>2</sub> + ... + w<sub>n</sub>x<sub>n</sub>+ b

<p align="center"><img src="https://user-images.githubusercontent.com/46274774/83376888-d2b81c00-a40e-11ea-92d6-078104da9934.png" width="50%"></p>

# 아달린
1. 아달린(Adaline)은 퍼샙트론은 개선한 적응형 선형뉴런(Adaptive Linear Neuron)으로 선형함수의 결과를 학습에 사용하며, 계단함수의 결과는 예측에만 활용한다.
2. 아달린의 퍼셉트론의 차이는 선형함수의 결과를 학습에 사용하는가 계단함수의 결과를 학습에 사용하는가 이다.
3. 로지스틱 회귀는 아달린의 개선 버전이며 아달린의 나머지 구조는 퍼셉트론과 동일하다.

# 로지스틱 회귀 (이름은 회귀이지만 분류 알고리즘이다!!)
1. 선형 함수를 통과하여 얻은 z를 임계함수에 보내기 전에 활성화 함수(Activation function)을 이용하여 변형시킨다.
2. 마지막 단계에서 임계 함수(threshold function)를 사용하여 예측을 수행한다.
3. 임계 함수(threshold function)은 아달린이나 퍼셉트론의 계단 함수와 역할은 비슷하지만, 활성화 함수의 출력을 사용한다는 점이 다르다.
4. 활성화 함수는 보통 비 선형의 함수를 사용한다  ==> 선형 함수를 사용하면 결국 임계 함수앞에 뉴런을 쌓고 z를 계속 입력으로 받고 출력을 쌓아도 선형 함수가 되므로 별 의미가 없다.

# 시그모이드 함수
1. 선형함수의 출력값 z를 0과 1사이의 확률값으로 변환시켜주는 함수
2. 보통 a가 0.5보다 크면 양성 클래스, 작으면 음성 클래스라고 구분한다.
3. 만들어지는 과정 : 오즈 비 --> 로짓 함수 --> 시그모이드 함수

# 오즈 비(odds ratio)
1. 성공확률과 실패 확률의 비율을 나타내는 통계 ==> OR = p/1-p(p=성공 확률)
2. 오즈 비는 처음에는 천천히 증가하지만 p의 값이 1에 가까워지면 급격히 증가한다.

<p align="center"><img src="https://user-images.githubusercontent.com/46274774/83377113-8f11e200-a40f-11ea-9d70-790f075274bd.png" width="30%"></p>

# 로짓 함수(logit function)
1. 오즈 비에 로그(자연 로그)를 취하여 만든 함수이다.
2. logit(p) = log(p/1-p)
3. p가 0.5이면 0이되고, 0과 1일때 각각 무한대로 음수와 양수가 되는 성질을 가지고 있다.
4. y축이 z라면 log(p/1-p) = z가 된다

<p align="center"><img src="https://user-images.githubusercontent.com/46274774/83377307-237c4480-a410-11ea-9874-ff8c689d0927.png" width="30%"></p>

# 로지스틱 함수(sigmoid function)
1. 위 식을 z에 대해 정리한 p = 1/1+e^-z 이며 이는 x축을 z로 두기 위함이다.
2. 로지스틱 함수는 로짓 함수의 가로축과 세로축을 뒤집은 모양이 되며 이는 S자 모양이 된다.
3. 이 모양에서 착안하여 로지스틱 함수를 시그모이드 함수라고 한다.

<p align="center"><img src="https://user-images.githubusercontent.com/46274774/83377332-3a229b80-a410-11ea-8c0d-fb373d0a70b1.png" width="30%"></p>

# 로지스틱 회귀
1. 로지스틱 회귀는 이진 분류가 목표이며, -&infin; ~ &infin;의 범위를 가지는 z의 값을 조절할 방법이 필요해서 시그모이드 함수를 활성 함수로 사용한 것이다.
 ==> 시그모이드 함수를 통과하면 이를 확률쳐럼 사용할 수 있기 때문이다.
2. 여기서 나온 확률을 임계 함수에 넣으면 0 또는 1의 값을 주며 이를 이용하여 이진 분류를 한다.

* 분류의 목표는 바로 올바르게 분류된 샘플 데이터의 비율 자체를 높이는 것이다.
* 올바르게 분류된 샘플의 비율은 미분 가능한 함수가 아니어서, 경사 하강법의 손실함수로 사용할 수 없다. ==> 로지스틱 손실 함수 사용

# 로지스틱 손실 함수
1. 다중 분류를 위한 손실 함수인 크로스 엔트로피(cross entropy)손실 함수를 이진 분류 버전으로 만든 것이다.
2. L = -(y*log(a) + (1-y)log(1-a)) 여기서 a는 활성화 함수가 출력한 값이며, y는 타깃이다.
3. 양성 클래스인 경우 로지스틱 손실 함수값의 크기가 최소가 되려면 a는 1에 가까워진다.
4. 음성 클래스의 경우 로지스틱 손실 함수값의 크기가 최소가 되려면 a는 0에 가까워진다.
5. 즉 로지스틱 손실 함수를 최소화 하면 a의 값이 우리가 가장 이상적으로 생각하는 값이 된다.

* 로지스틱 소실 함수의 미분을 통해 로지스틱 손실 함수의 값을 최소로 하는 가중치와 절편을 찾아야 한다.
* 계산에는 미분의 연쇄법칙을 따르며, L은 a에대해, a는 z에 대해, z는 w에 대해 미분해야 한다.
* 여기서 a는 시그모이드 함수이다.

* 미분의 결과는 로지스틱 손실 함수를 w에 대해 미분한 결과는 제곱 오차를 미분한 결과와 일치한다.

* 로지스틱 손실 함수에 대한 미분이 연쇄법칙에 의해 진행되는 구조를 보고 '그레디언트가 역전파 된다'라고한다.

* 가중치의 업데이트 : 로지스틱 회귀의 가중치는 로지스틱 손실 함수를 가중치에 대해 미분하고 빼주면 된다.
* 절편의 업데이트 : 위와 같이 로지스틱 손실 함수를 절편에 대해 미분한 식을 가지고 절편에서 빼면 된다.

# 모델의 성능 평가
* 훈련된 모델의 실전 성능을 일반화 성능(generalization performance)라고 한다
* 훈련데이터로 훈련된 모델을 평가하는 것을 '과도하게 낙관적으로 일반화 성능을 추정한다'라고 하며 당연히 높은 성능이 나올것이다.
* 따라서 훈련뒤에 평가하기 위해 test와 train으로 데이터를 나눈다

# 훈련 데이터 세트를 훈련 세트와 테스트 세트로 나누는 규칙
1. 훈련 데이터 세트를 나눌때는 테스트 세트보다 훈련 세트가 많아야 한다.
2. 훈련 데이터 세트를 나누기 전에 양성, 음성 클래스가 훈련 세트나 테스트 세트 어느 한쪽에 몰리지 않도록 골고루 섞어야 한다.

# 일반적인 신경망
* 왼쪽에 입력층(Input Layer) 오른쪽에 출력층(Output Layer) 가운데는 은닉층(Hidden Layer)라고 한다.
* 활성화 함수는 은닉층과 출력층에 한 부분으로 간주한다.

# 단일층 신경망
* 방금 만든 로지스틱 회귀는 은닉층이 없는 단일층 신경망이다.
* 선형 회귀와 로지스틱 회귀는 모두 경사 하강법을 사용하였다.
* 경사 하강법은 손실함수(제곱 오차 손실 함수, 로지스틱 손싱 함수)의 결과값을 최소화 하도록 가중치를 업데이트 한다.

# 손실함수의 결과값을 조정해 저장 기능 추가
* np.clip()은 주어진 범위 밖의 값을 범위 양끝의 값으로 잘라낸다.

# 확률적 경사 하강법(stochastic gradient descent)
* 샘플 데이터 1개에 대한 그레디언트 계산한다.(1개의 샘플을 중복되지 않게 무작위로 선택한다.)
* 샘플 데이터 1개에 대해 그레디언트를 계산하므로 비용이 적지만 최적값에 수렴하는과정이 불안정하다.

# 배치 경사 하강법(batch gradient descent)
* 전체 훈련 세트를 활용하여 한 번에 그레디언트를 계산한다.
* 전체 훈련 세트를 사용하여 한 번에 그레디언트를 계산하므로 가중치가 최적값에 수렴하는 과정이 안정적이지만, 그만큼 비용이 많이든다.

# 미니 배치 경사 하강법(mini-batch gradient descent)
* 훈련 세트를 여러개로 나누어 그레디언트를 계산한다.(전체 샘플 중 몇개의 샘플을 중복되지 않도록 무작위로 선택.)

# 훈련데이터 섞기
* 모든 경사 하강법들은 매 에포크마다 훈련 세트의 샘플 순서를 섞어 가중치의 최적값을 계산한다.
* 샘플의 순서를 섞으면 가중치 최적값의 탐색 과정이 다양해져 가중치의 최적값을 잘 찾을 수 있다.
* 훈련 세트의 샘플의 순서를 섞는 방법으 배열의 인덱스를 섞어주고 인덱스를 뽑아주는 것이다.

